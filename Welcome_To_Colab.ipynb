{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadsahad/Top-100-Coders/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***POS TAGGING USING HMM***"
      ],
      "metadata": {
        "id": "2hqGKyei8QxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm_tagger():\n",
        "  nltk_tagger=nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "  tagger=nltk_tagger.train_tagger(brown.tagged.sents())\n",
        "  return tagger\n",
        "def pos_tag(sentence,tagger):\n",
        "  tokens=nltk.word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=train_hmm_tagger()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "qTBCwsd_8dPj",
        "outputId": "c7155121-3d53-413d-aa63-f50125f26d25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HiddenMarkovModelTrainer' object has no attribute 'train_tagger'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2015afde9608>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hmm_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a sentence: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhmm_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2015afde9608>\u001b[0m in \u001b[0;36mtrain_hmm_tagger\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_hmm_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mnltk_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddenMarkovModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HiddenMarkovModelTrainer' object has no attribute 'train_tagger'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDRDndO394mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm_tagger():\n",
        "    nltk_tagger = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "    tagger = nltk_tagger.train(brown.tagged_sents())\n",
        "    return tagger\n",
        "\n",
        "def pos_tag(sentence, tagger):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = tagger.tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "hmm_tagger = train_hmm_tagger()\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "tagged = pos_tag(sentence, hmm_tagger)\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-gDrsAy-gxH",
        "outputId": "695626ab-7ca1-4450-d903-13244611087a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: The sky is so beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('so', 'QL'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm_tagger():\n",
        "    # Use the 'news' category from the Brown corpus\n",
        "    news_tagged_sents = brown.tagged_sents(categories='news')\n",
        "\n",
        "    # Extract the set of states (POS tags) and symbols (words)\n",
        "    symbols = set(word for sent in news_tagged_sents for word, tag in sent)\n",
        "    states = set(tag for sent in news_tagged_sents for word, tag in sent)\n",
        "\n",
        "    # Create and train the HMM tagger\n",
        "    trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=states, symbols=symbols)\n",
        "    tagger = trainer.train(news_tagged_sents)\n",
        "    return tagger\n",
        "\n",
        "def pos_tag(sentence, tagger):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = tagger.tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "hmm_tagger = train_hmm_tagger()\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "tagged = pos_tag(sentence, hmm_tagger)\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFqU2wxO-hU4",
        "outputId": "cb97afee-ab58-4983-8db7-e847f7319480"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: The sky is so beautiful\n",
            "[('The', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('so', 'QL'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def hmm_trainer():\n",
        "  news_tagged_sents=brown.tagged_sents(categories='news')\n",
        "  states=set(tag for sents in news_tagged_sents for word,tag in sents)\n",
        "  symbols=set(word for sents in news_tagged_sents for word,tag in sents)\n",
        "  trainer=nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
        "  tagger=trainer.train(news_tagged_sents)\n",
        "  return tagger\n",
        "\n",
        "def pos_tagger(sentence,tagger):\n",
        "  tokens=nltk.word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=hmm_trainer()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jjs-j7AALgx",
        "outputId": "5a9dfb56-cbb8-4629-f20b-27673e661a04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: The sky is beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm():\n",
        "  news_tagged_sents=brown.tagged_sents(categories='news')\n",
        "  symbols=set(word for sents in news_tagged_sents for word,tag in sents)\n",
        "  states=set(tag for sents in news_tagged_sents for word,tag in sents)\n",
        "  trainer=nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
        "  tagger= trainer.train(news_tagged_sents)\n",
        "  return tagger\n",
        "\n",
        "\n",
        "def pos_tag(sentence,tagger):\n",
        "  tokens=nltk.word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=train_hmm()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HYWpIWBByTt",
        "outputId": "8e7c40e4-43f3-4da8-f5a0-cf4d1356027f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: the sky is beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Naive Bayes Classifier***"
      ],
      "metadata": {
        "id": "rDx_gMdoQ69U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return dict(nltk.FreqDist(tokens))\n",
        "\n",
        "# Load and label the movie reviews dataset\n",
        "pos_reviews = [(movie_reviews.raw(fileid), 'positive') for fileid in movie_reviews.fileids('pos')]\n",
        "neg_reviews = [(movie_reviews.raw(fileid), 'negative') for fileid in movie_reviews.fileids('neg')]\n",
        "tot_reviews = pos_reviews + neg_reviews\n",
        "\n",
        "# Process the data\n",
        "processed_data = [(preprocess(text), category) for text, category in tot_reviews]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "# Classify new text\n",
        "new_text = [\"The movie is amazing\", \"This movie is terrible\", \"The movie was awful\"]\n",
        "for text in new_text:\n",
        "    print(f\"Text: '{text}' -> Sentiment: {classifier.classify(preprocess(text))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmQQimdHUJaD",
        "outputId": "6d9f168c-2ba8-4e61-aaa7-76c0013be6aa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'The movie is amazing' -> Sentiment: positive\n",
            "Text: 'This movie is terrible' -> Sentiment: negative\n",
            "Text: 'The movie was awful' -> Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LOGISTIC REGRESSION***"
      ],
      "metadata": {
        "id": "9HNMCb49v3Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess(text):\n",
        "    ps = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [ps.stem(word) for word in words if word not in stop_words and word.isalpha()]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "sentences = [\"The food is tasty\", \"the quality of the food is low\", \"i will never recommend their food\",\n",
        "             \"i got sick after having their food\", \"I was in cloudnine after tasting their food\",\n",
        "             \"My favourite is their desserts\", \"the food was not cooked properly\"]\n",
        "classes = [1, 0, 0, 0, 1, 1, 0]\n",
        "test_sentences = [\"food is not cooked properly\", \"i feel sick after having their food\", \"I love their desserts\",\n",
        "                  \"was in cloudnine after tasting their food\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Preprocess the sentences\n",
        "preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
        "vect1 = vectorizer.fit_transform(preprocessed_sentences)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "nb = LogisticRegression()\n",
        "nb.fit(vect1, classes)\n",
        "\n",
        "# Preprocess the test sentences\n",
        "preprocessed_test_sentences = [preprocess(sentence) for sentence in test_sentences]\n",
        "vect2 = vectorizer.transform(preprocessed_test_sentences)\n",
        "\n",
        "# Predict the classes for the test sentences\n",
        "pred_classes = nb.predict(vect2)\n",
        "print(pred_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-C6jgZC2PcD",
        "outputId": "79eaf835-1339-4429-95a6-15d81a34a938"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LR***"
      ],
      "metadata": {
        "id": "zKBL7rRphnsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  ps=PorterStemmer()\n",
        "  words=nltk.word_tokenize(text.lower())\n",
        "  filtered_words=[ps.stem(word)for word in words if word not in stop_words and word.isalpha()]\n",
        "  return ' '.join(filtered_words)\n",
        "\n",
        "sentences=[\"The food is tasty\",\"the quality of food is low\",\"I will never recommend their food\",\n",
        "           \"I feel sick after tasting their food\",\"My favourite food is their desserts\",\"i went cloudnine after tasting their food\",\n",
        "           \"the food was not cooked properly\"]\n",
        "classes=[1,0,0,0,1,1,0]\n",
        "test_sentence=[\"the food is not cooked properly\",\"i feel sick after having their food\",\"i love their desserts\",\"went cloudnine after tasting their food\"]\n",
        "\n",
        "vectorizer=CountVectorizer()\n",
        "\n",
        "preprocessed_sentences=[preprocess(sentence)for sentence in sentences]\n",
        "vect1=vectorizer.fit_transform(preprocessed_sentences)\n",
        "\n",
        "nb=LogisticRegression()\n",
        "nb.fit(vect1,classes)\n",
        "\n",
        "preprocessed_test_sentences=[preprocess(sentence)for sentence in test_sentences]\n",
        "vect2=vectorizer.transform(preprocessed_test_sentences)\n",
        "\n",
        "pred=nb.predict(vect2)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcsVs4G9hkdW",
        "outputId": "95b3f88b-d428-459c-a62f-20bac6ded5da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Naive Bayes***"
      ],
      "metadata": {
        "id": "TP6R4AO5nvRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "  tokens=word_tokenize(text.lower())\n",
        "  filtered_tokens=[lemmatizer.lemmatize(token)for token in tokens if token not in stop_words and token.isalpha()]\n",
        "  return dict(nltk.FreqDist(filtered_tokens))\n",
        "\n",
        "pos_reviews=[(movie_reviews.raw(fileid),'positive')for fileid in movie_reviews.fileids('pos')]\n",
        "neg_reviews=[(movie_reviews.raw(fileid),'negative')for fileid in movie_reviews.fileids('neg')]\n",
        "tot_reviews=pos_reviews+neg_reviews\n",
        "\n",
        "processed_text=[(preprocess(text),category)for (text,category) in tot_reviews]\n",
        "train_data,val_data=train_test_split(processed_text,test_size=0.2,random_state=42)\n",
        "\n",
        "classifier=NaiveBayesClassifier.train(train_data)\n",
        "new_text=[\"The movie was awful\",\"The movie was boring\",\"The movie was amazing\"]\n",
        "for text in new_text:\n",
        "  new_features=preprocess(text)\n",
        "  prediction_category=classifier.classify(new_features)\n",
        "  print(f\"Text:'{text}'->sentiment:'{prediction_category}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UTh8uIbn0fc",
        "outputId": "3a96b90b-201e-4f43-dfec-9403e5edb950"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:'The movie was awful'->sentiment:'negative'\n",
            "Text:'The movie was boring'->sentiment:'negative'\n",
            "Text:'The movie was amazing'->sentiment:'positive'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***POS Tagging using HMM***"
      ],
      "metadata": {
        "id": "j_evlnrdtlUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def train_hmm_tagger():\n",
        "  nltk_tagger=nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "  tagger=nltk_tagger.train(brown.tagged_sents())\n",
        "  return tagger\n",
        "\n",
        "def pos_tag(sentence,tagger):\n",
        "  tokens=word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=train_hmm_tagger()\n",
        "sentence=input('Enter a sentence: ')\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "ToxR9nq0sKP9",
        "outputId": "8ae7f004-2ae3-4e78-bc9c-ecdec26b4ffa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: the sky is beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0cd7bdf57e0a>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hmm_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter a sentence: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtagged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhmm_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-0cd7bdf57e0a>\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(sentence, tagger)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hmm_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, unlabeled_sequence)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m    287\u001b[0m         \u001b[0munlabeled_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py\u001b[0m in \u001b[0;36m_tag\u001b[0;34m(self, unlabeled_sequence)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py\u001b[0m in \u001b[0;36m_best_path\u001b[0;34m(self, unlabeled_sequence)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py\u001b[0m in \u001b[0;36m_create_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                     \u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_logprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py\u001b[0m in \u001b[0;36m_output_logprob\u001b[0;34m(self, state, symbol)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \"\"\"\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/probability.py\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# Default definition, in terms of prob()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/probability.py\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freqdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def train_hmm_tagger():\n",
        "  news_tagged_sents=brown.tagged_sents(categories='news')\n",
        "  symbols=set(word for sents in news_tagged_sents for word,tag in sents)\n",
        "  states=set(tag for sents in news_tagged_sents for word,tag in sents)\n",
        "  trainer=nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
        "  tagger=trainer.train(news_tagged_sents)\n",
        "  return tagger\n",
        "\n",
        "def pos_tag(sentence,tagger):\n",
        " tokens=word_tokenize(sentence.lower())\n",
        " tagged_tokens=tagger.tag(tokens)\n",
        " return tagged_tokens\n",
        "\n",
        "hmm_tagger=train_hmm_tagger()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-5ZuP4wxeIY",
        "outputId": "baa9dbad-6f2d-44a2-c3fc-50a319b73723"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: the sky is beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***CHATBOT***"
      ],
      "metadata": {
        "id": "o59qeOLT4ZiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chat.util import Chat,reflections\n",
        "\n",
        "pairs=[[r\"Hi|hello|hola\",[\"Hello,i am aura your ai assistant...\"]],\n",
        "       [r\"how are you?\",[\"i am good.what about you?\"]],\n",
        "       [r\"quit\",[\"goodbye\"]],\n",
        "       [r\"(.*)\",[\"could you try again\"]]]\n",
        "bot=Chat(pairs,reflections)\n",
        "bot.converse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv5EPSov1aut",
        "outputId": "0bdef040-5714-4fdf-e7a6-e8d4496a13a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">hi\n",
            "Hello,i am aura your ai assistant...\n",
            ">how are you\n",
            "i am good.what about you?\n",
            ">what\n",
            "could uou try again\n",
            ">oksy\n",
            "could uou try again\n",
            ">quit\n",
            "goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqoFn4og5QEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}