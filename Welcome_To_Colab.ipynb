{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadsahad/Top-100-Coders/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***POS TAGGING USING HMM***"
      ],
      "metadata": {
        "id": "2hqGKyei8QxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm_tagger():\n",
        "  nltk_tagger=nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "  tagger=nltk_tagger.train_tagger(brown.tagged.sents())\n",
        "  return tagger\n",
        "def pos_tag(sentence,tagger):\n",
        "  tokens=nltk.word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=train_hmm_tagger()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "id": "qTBCwsd_8dPj",
        "outputId": "c7155121-3d53-413d-aa63-f50125f26d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HiddenMarkovModelTrainer' object has no attribute 'train_tagger'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2015afde9608>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hmm_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a sentence: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhmm_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2015afde9608>\u001b[0m in \u001b[0;36mtrain_hmm_tagger\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_hmm_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mnltk_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddenMarkovModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HiddenMarkovModelTrainer' object has no attribute 'train_tagger'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDRDndO394mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm_tagger():\n",
        "    nltk_tagger = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
        "    tagger = nltk_tagger.train(brown.tagged_sents())\n",
        "    return tagger\n",
        "\n",
        "def pos_tag(sentence, tagger):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = tagger.tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "hmm_tagger = train_hmm_tagger()\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "tagged = pos_tag(sentence, hmm_tagger)\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "id": "V-gDrsAy-gxH",
        "outputId": "695626ab-7ca1-4450-d903-13244611087a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: The sky is so beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('so', 'QL'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm_tagger():\n",
        "    # Use the 'news' category from the Brown corpus\n",
        "    news_tagged_sents = brown.tagged_sents(categories='news')\n",
        "\n",
        "    # Extract the set of states (POS tags) and symbols (words)\n",
        "    symbols = set(word for sent in news_tagged_sents for word, tag in sent)\n",
        "    states = set(tag for sent in news_tagged_sents for word, tag in sent)\n",
        "\n",
        "    # Create and train the HMM tagger\n",
        "    trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=states, symbols=symbols)\n",
        "    tagger = trainer.train(news_tagged_sents)\n",
        "    return tagger\n",
        "\n",
        "def pos_tag(sentence, tagger):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = tagger.tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "hmm_tagger = train_hmm_tagger()\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "tagged = pos_tag(sentence, hmm_tagger)\n",
        "print(tagged)\n"
      ],
      "metadata": {
        "id": "dFqU2wxO-hU4",
        "outputId": "cb97afee-ab58-4983-8db7-e847f7319480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: The sky is so beautiful\n",
            "[('The', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('so', 'QL'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def hmm_trainer():\n",
        "  news_tagged_sents=brown.tagged_sents(categories='news')\n",
        "  states=set(tag for sents in news_tagged_sents for word,tag in sents)\n",
        "  symbols=set(word for sents in news_tagged_sents for word,tag in sents)\n",
        "  trainer=nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
        "  tagger=trainer.train(news_tagged_sents)\n",
        "  return tagger\n",
        "\n",
        "def pos_tagger(sentence,tagger):\n",
        "  tokens=nltk.word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=hmm_trainer()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "id": "3Jjs-j7AALgx",
        "outputId": "5a9dfb56-cbb8-4629-f20b-27673e661a04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: The sky is beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def train_hmm():\n",
        "  news_tagged_sents=brown.tagged_sents(categories='news')\n",
        "  symbols=set(word for sents in news_tagged_sents for word,tag in sents)\n",
        "  states=set(tag for sents in news_tagged_sents for word,tag in sents)\n",
        "  trainer=nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
        "  tagger= trainer.train(news_tagged_sents)\n",
        "  return tagger\n",
        "\n",
        "\n",
        "def pos_tag(sentence,tagger):\n",
        "  tokens=nltk.word_tokenize(sentence)\n",
        "  tagged_tokens=tagger.tag(tokens)\n",
        "  return tagged_tokens\n",
        "hmm_tagger=train_hmm()\n",
        "sentence=input(\"Enter a sentence: \")\n",
        "tagged=pos_tag(sentence,hmm_tagger)\n",
        "print(tagged)"
      ],
      "metadata": {
        "id": "2HYWpIWBByTt",
        "outputId": "8e7c40e4-43f3-4da8-f5a0-cf4d1356027f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: the sky is beautiful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'AT'), ('sky', 'NN'), ('is', 'BEZ'), ('beautiful', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Naive Bayes Classifier***"
      ],
      "metadata": {
        "id": "rDx_gMdoQ69U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return dict(nltk.FreqDist(tokens))\n",
        "\n",
        "# Load and label the movie reviews dataset\n",
        "pos_reviews = [(movie_reviews.raw(fileid), 'positive') for fileid in movie_reviews.fileids('pos')]\n",
        "neg_reviews = [(movie_reviews.raw(fileid), 'negative') for fileid in movie_reviews.fileids('neg')]\n",
        "tot_reviews = pos_reviews + neg_reviews\n",
        "\n",
        "# Process the data\n",
        "processed_data = [(preprocess(text), category) for text, category in tot_reviews]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "# Classify new text\n",
        "new_text = [\"The movie is amazing\", \"This movie is terrible\", \"The movie was awful\"]\n",
        "for text in new_text:\n",
        "    print(f\"Text: '{text}' -> Sentiment: {classifier.classify(preprocess(text))}\")\n"
      ],
      "metadata": {
        "id": "HmQQimdHUJaD",
        "outputId": "6d9f168c-2ba8-4e61-aaa7-76c0013be6aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'The movie is amazing' -> Sentiment: positive\n",
            "Text: 'This movie is terrible' -> Sentiment: negative\n",
            "Text: 'The movie was awful' -> Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LOGISTIC REGRESSION***"
      ],
      "metadata": {
        "id": "9HNMCb49v3Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess(text):\n",
        "    ps = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [ps.stem(word) for word in words if word not in stop_words and word.isalpha()]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "sentences = [\"The food is tasty\", \"the quality of the food is low\", \"i will never recommend their food\",\n",
        "             \"i got sick after having their food\", \"I was in cloudnine after tasting their food\",\n",
        "             \"My favourite is their desserts\", \"the food was not cooked properly\"]\n",
        "classes = [1, 0, 0, 0, 1, 1, 0]\n",
        "test_sentences = [\"food is not cooked properly\", \"i feel sick after having their food\", \"I love their desserts\",\n",
        "                  \"was in cloudnine after tasting their food\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Preprocess the sentences\n",
        "preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
        "vect1 = vectorizer.fit_transform(preprocessed_sentences)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "nb = LogisticRegression()\n",
        "nb.fit(vect1, classes)\n",
        "\n",
        "# Preprocess the test sentences\n",
        "preprocessed_test_sentences = [preprocess(sentence) for sentence in test_sentences]\n",
        "vect2 = vectorizer.transform(preprocessed_test_sentences)\n",
        "\n",
        "# Predict the classes for the test sentences\n",
        "pred_classes = nb.predict(vect2)\n",
        "print(pred_classes)\n"
      ],
      "metadata": {
        "id": "F-C6jgZC2PcD",
        "outputId": "79eaf835-1339-4429-95a6-15d81a34a938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}